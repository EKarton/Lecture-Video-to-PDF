1
00:00:00,000 --> 00:00:04,528
The next thing we're going to look at is
how to compute modular large integers. So

2
00:00:04,528 --> 00:00:09,023
the first question is how do we represent
large integers in a computer? So that's

3
00:00:09,023 --> 00:00:13,615
actually fairly straightforward. So
imagine we're on a 64 bit machine, what we

4
00:00:13,615 --> 00:00:18,361
would do is we would break the number we
want to represent, into 32 bit buckets And

5
00:00:18,361 --> 00:00:22,686
then, we will basically have these n/32 bit buckets, and together they will

6
00:00:22,686 --> 00:00:26,906
represent the number that we want to store
on the computer. Now, I should mention

7
00:00:26,906 --> 00:00:30,705
that I'm only giving 64 bit registers as
an example. In fact, many modern

8
00:00:30,705 --> 00:00:34,977
processors have 128 bit registers or more,
and you can even do multiplications on

9
00:00:34,977 --> 00:00:38,987
them. So normally you would actually use
much larger blocks than just 32 bits. The

10
00:00:38,987 --> 00:00:42,943
reason, by the way, you want to limit
yourself to 32 bits is so that you can

11
00:00:42,943 --> 00:00:46,952
multiply two blocks together, and the
result will still be less than 64 bits,

12
00:00:46,952 --> 00:00:51,189
less than the word size on the machine. So
now let's look at particular arithmetic

13
00:00:51,189 --> 00:00:54,788
operations and see how long each one
takes. So addition and subtraction

14
00:00:54,788 --> 00:00:58,742
basically what you would do is that
addition would carry or subtraction would

15
00:00:58,742 --> 00:01:03,000
borrow and those are basically linear time
operations. In other words, if you want to

16
00:01:03,000 --> 00:01:06,954
add or subtract two n bit integers the
running time is basically

17
00:01:06,954 --> 00:01:12,626
linear in n. Multiplication
naively will take quadratic time. In fact,

18
00:01:12,626 --> 00:01:16,676
this is what's called the high school
algorithm. This is what you kind of

19
00:01:16,676 --> 00:01:21,114
learned in school, where if you think
about this for a minute you'll see that,

20
00:01:21,114 --> 00:01:25,662
that algorithm basically is quadratic in
the length of the numbers that are being

21
00:01:25,662 --> 00:01:30,156
multiplied. So a big surprise in the 1960s
was an algorithm due to Karatsuba that

22
00:01:30,156 --> 00:01:34,150
actually achieves much better than
quadratic time in fact, it achieved a

23
00:01:34,150 --> 00:01:38,567
running time of n to the 1.585. And
there's actually no point in me showing

24
00:01:38,567 --> 00:01:43,166
you how the algorithm actually worked,
I'll just mention the main idea What

25
00:01:43,166 --> 00:01:48,071
Karatsuba realized, is that in fact when
you want to multiply two numbers, you can

26
00:01:48,071 --> 00:01:52,976
write them as, you can take the first
number x, write it as 2 to the b times

27
00:01:52,976 --> 00:01:57,882
x2 plus x1. Where x2 and x1 are roughly
the size of the square root of x. Okay, so

28
00:01:57,882 --> 00:02:02,910
we can kind of break the number x into the
left part of x and the right part of x.

29
00:02:02,910 --> 00:02:07,654
And basically, you're writing x as if it
was written base 2 to the b. So it's got

30
00:02:07,654 --> 00:02:12,398
two digits base 2 to the b. And you do
the same thing with, y. You write y base

31
00:02:12,398 --> 00:02:16,911
2 to the b. Again, you would write it
as, the sum of the left half plus the

32
00:02:16,911 --> 00:02:21,540
right half, And then, normally, if you try
to do this multiplication, when you open

33
00:02:21,540 --> 00:02:27,486
up the parentheses. You see that, this
would require 4 multiplications, right?

34
00:02:27,486 --> 00:02:33,365
It would require x2 times y2, x2 times y1,
x1 times y2, and x1 times y1. What

35
00:02:33,365 --> 00:02:39,879
Karatsuba realized is there's a way to do
this multiplication of x by y using only

36
00:02:39,879 --> 00:02:45,847
three multiplications of x1 x2 y1 y2. So it's just a big multiplication of x times y

37
00:02:45,847 --> 00:02:50,214
only it takes three little multiplications. You can now recursively

38
00:02:50,214 --> 00:02:55,087
apply exactly the same procedure to
multiplying x2 by y2, and x2 by y1, and so

39
00:02:55,087 --> 00:02:59,960
on and so forth. And you would get this
recursive algorithm. And if you do the

40
00:02:59,960 --> 00:03:05,087
recursive analysis, you will see that
basically, you get a running time of n to the 1.585.

41
00:03:05,087 --> 00:03:13,640
This number is basically, the 1.585 is basically, log of 3 base 2.

42
00:03:13,640 --> 00:03:17,836
Surprisingly, it turns out that Karatsuba
isn't even the best multiplication

43
00:03:17,836 --> 00:03:23,912
algorithm out there. It turns out that, in fact, you can do multiplication in about nlog(n) time.

44
00:03:23,912 --> 00:03:28,678
So you can do multiplication in almost linear time. However, this is an extremely asymptotic results.

45
00:03:28,678 --> 00:03:31,477
The big O here hides very big constants. And as a

46
00:03:31,477 --> 00:03:35,452
result, this algorithm only becomes
practical when the numbers are absolutely

47
00:03:35,452 --> 00:03:39,152
enormous. And so this algorithm is
actually not used very often. But

48
00:03:39,152 --> 00:03:43,183
Karatsuba's algorithm is very practical.
And in fact, most crypto-libraries

49
00:03:43,183 --> 00:03:47,885
implement Karatsuba's algorithm for
multiplication. However, for simplicity

50
00:03:47,885 --> 00:03:51,923
here, I'm just gonna ignore Karatsuba's
algorithm, and just for simplicity, I'm

51
00:03:51,923 --> 00:03:55,960
gonna assume that multiplication runs in
quadratic time. But in your mind, you

52
00:03:55,960 --> 00:03:59,893
should always be thinking all multiplication really is a little bit faster than quadratic.

53
00:03:59,893 --> 00:04:04,794
And then the next question after multiplication is what about

54
00:04:04,794 --> 00:04:10,297
division with remainder and it turns out
that's also a quadratic time algorithm.

55
00:04:10,297 --> 00:04:15,420
So the main operation that remains, and one
that we've used many times so far, and

56
00:04:15,420 --> 00:04:20,346
I've never, actually never, ever told you
how to actually compute it, is this

57
00:04:20,346 --> 00:04:26,339
question of exponentiation. So let's solve
this exponentiation problem a bit more

58
00:04:26,339 --> 00:04:31,558
abstractly. So imagine we have a finite
cyclic group G. All this means is that

59
00:04:31,558 --> 00:04:37,115
this group is generated from the powers of
some generator little g. So for example

60
00:04:37,115 --> 00:04:42,673
think of this group as simply ZP, and think of little g as some generator of

61
00:04:42,673 --> 00:04:48,886
big G. The reason I'm sitting in this way,
is I'm, I want you to start getting used

62
00:04:48,886 --> 00:04:54,023
to this abstraction where we deal with a
generic group G and ZP really is just

63
00:04:54,023 --> 00:04:58,915
one example of such a group. But, in fact,
there are many other examples of finite

64
00:04:58,915 --> 00:05:03,379
cyclic groups. And again I want to
emphasis basically that group G, all it

65
00:05:03,379 --> 00:05:08,087
is, it's simply this powers of this
generator up to the order of the group.

66
00:05:08,087 --> 00:05:15,153
I'll write it as G to the Q. So our goal
now is given this element g, and some

67
00:05:15,153 --> 00:05:20,797
exponent x, our goal is to compute the
value of g to the x. Now normally what you

68
00:05:20,797 --> 00:05:24,810
would say is, you would think well, you
know, if x is equal to 3  then I'm

69
00:05:24,810 --> 00:05:28,898
gonna compute you know, g cubed. Well,
there's really nothing to do. All I do is

70
00:05:28,898 --> 00:05:32,795
I just do g times g times g and I get g
cubed, which is what I wanted. So that's

71
00:05:32,795 --> 00:05:36,790
actually pretty easy. But in fact, that's
not the case that we're interested in. In

72
00:05:36,790 --> 00:05:40,638
our case, our exponents are gonna be
enormous. And so if you try, you know,

73
00:05:40,638 --> 00:05:45,644
think of like a 500-bit number and so if
you try to compute g to the power of a

74
00:05:45,644 --> 00:05:50,710
500-bit number simply by multiplying g by
g by g by g this is gonna take quite a

75
00:05:50,710 --> 00:05:55,716
while. In fact it will take exponential
time which is not something that we want

76
00:05:55,897 --> 00:06:00,722
to do. So the question is whether even
though x is enormous, can we still compute

77
00:06:00,722 --> 00:06:05,667
g to the x relatively fast and the answer
is yes and the algorithm that does that

78
00:06:05,667 --> 00:06:10,822
is called a repeated squaring algorithm.
And so let me show you how repeated

79
00:06:10,822 --> 00:06:15,593
squaring works. So let's take as an
example, 53. Naively you would have to do

80
00:06:15,593 --> 00:06:20,295
53 multiplications of g by g by g by g
until you get to g by the 53 but I want to

81
00:06:20,295 --> 00:06:25,275
show you how you can do it very quickly.
So what we'll do is we'll write 53 in

82
00:06:25,275 --> 00:06:30,497
binary. So here this is the binary
representation of 53. And all that means

83
00:06:30,497 --> 00:06:36,282
is, you notice this one corresponds to 32,
this one corresponds to 16, this one

84
00:06:36,282 --> 00:06:41,292
corresponds to 4, and this one
corresponds to 1. So really 53 is 32

85
00:06:41,292 --> 00:06:47,038
plus 16 plus 4 plus 1. But what
that means is that g to the power of 53 is

86
00:06:47,038 --> 00:06:51,801
g to the power of 32+16+4+1. And we can
break that up, using again, the rules of

87
00:06:51,801 --> 00:06:57,235
exponentiation. We can break that up as g
to the 32 times g to the 16 times g to the

88
00:06:57,235 --> 00:07:02,938
4 times g to the 1, Now that should start
giving you an idea for how to compute g to

89
00:07:02,938 --> 00:07:07,141
the 53 very quickly. What we'll do is,
simply, we'll take g and we'll start

90
00:07:07,141 --> 00:07:11,459
squaring it. So what square wants, g wants
to get g squared. We square it again to

91
00:07:11,459 --> 00:07:15,778
get g to the 4, turn g to the 8.
Turn g to the 16, g to the 32. So

92
00:07:15,778 --> 00:07:20,607
we've computed all these squares of g. And
now, what we're gonna do is we're simply

93
00:07:20,607 --> 00:07:25,719
gonna multiply the appropriate powers to
give us the g to the 53. So this is g to

94
00:07:25,719 --> 00:07:30,390
the one times g to the 4 times g to the 16 times g to the 32, is basically

95
00:07:30,390 --> 00:07:35,376
gonna give us the value that we want,
which is g to the 53. So here you see that

96
00:07:35,376 --> 00:07:40,173
all we had to do was just compute, let's
see, we had to do one, two, three, four,

97
00:07:40,173 --> 00:07:49,343
five squaring, plus four more multiplications
so with 9 multiplications we computed g

98
00:07:49,343 --> 00:07:53,726
to the 53. Okay so that's pretty
interesting. And it turns out this is a

99
00:07:53,726 --> 00:07:58,253
general phenomena that allows us to raise
g to very, very high powers and do it very

100
00:07:58,253 --> 00:08:02,509
quickly. So let me show you the algorithm,
as I said this is called the repeated

101
00:08:02,509 --> 00:08:06,497
squaring algorithm. So the input to the
algorithm is the element g and the

102
00:08:06,497 --> 00:08:10,858
exponent x. And the output is g to the x.
So what we're gonna do is we're gonna

103
00:08:10,858 --> 00:08:15,415
write x in binary notation. So let's say
that x has n bits. And this is the actual

104
00:08:15,415 --> 00:08:19,521
bit representation of x as a binary
number. And then what we'll do is the

105
00:08:19,521 --> 00:08:24,246
following. We'll have these two registers.
y is gonna be a register that's constantly

106
00:08:24,246 --> 00:08:28,127
squared. And then z is gonna be an
accumulator that multiplies in the

107
00:08:28,127 --> 00:08:32,683
appropriate powers of g as needed. So all
we do is the following we loop through the

108
00:08:32,683 --> 00:08:36,526
bits of x starting from the least
significant bits, And then we do the

109
00:08:36,526 --> 00:08:41,414
following: in every iteration we're simply
gonna square y. Okay, so y just keeps on

110
00:08:41,414 --> 00:08:45,940
squaring at every iteration. And then
whenever the corresponding bit of the

111
00:08:45,940 --> 00:08:50,554
exponent x happens to be one, we simply
accumulate the current value of y into

112
00:08:50,554 --> 00:08:55,173
this accumulator z and then at the end, we
simply output z. That's it. That's the

113
00:08:55,173 --> 00:08:59,558
whole algorithm, and that's the repeated
squaring algorithm. So, let's see an

114
00:08:59,558 --> 00:09:04,060
example with G to the 53. So,
you can see the two columns. y is one

115
00:09:04,060 --> 00:09:08,387
column, as it evolves through the
iterations, and z is another column, again

116
00:09:08,387 --> 00:09:13,064
as it evolves through the iterations. So,
y is not very interesting. Basically, all

117
00:09:13,064 --> 00:09:17,449
that happens to y is that at every
iteration, it simply gets squared. And so

118
00:09:17,449 --> 00:09:22,299
it just walks through the powers of two
and the exponents and that's it. z is the

119
00:09:22,299 --> 00:09:26,915
more interesting register where what it
does is it accumulates the appropriate

120
00:09:26,915 --> 00:09:31,882
powers of g whenever the corresponding bit
to the exponent is one. So for example the

121
00:09:31,882 --> 00:09:36,031
first bit of the exponent is one,
therefore, the, at the end of the first

122
00:09:36,031 --> 00:09:41,219
iteration the value of z is simply equal to
g. The second bit of the exponent is zero

123
00:09:41,219 --> 00:09:46,473
so the value of z doesn't change after the
second iteration. And at the end of the

124
00:09:46,473 --> 00:09:51,856
third iteration well the third bit of the
exponent is one so we accumulate g to the

125
00:09:51,856 --> 00:09:56,662
fourth into z. The next bit of the
exponent is zero so z doesn't change. The

126
00:09:56,662 --> 00:10:02,109
next bit of the exponent is one and so now
we're supposed to accumulate the previous

127
00:10:02,109 --> 00:10:07,491
value of y into the accumulator z so let
me ask you so what's gonna be the value of z?

128
00:10:10,868 --> 00:10:14,245
Well, we simply accumulate g to the
16 into z and so we simply compute the sum

129
00:10:14,245 --> 00:10:19,594
of 16 and 5 we get g to the 21.
Finally, the last bit is also set to one

130
00:10:19,594 --> 00:10:24,943
so we accumulate it into z, we do 32 plus 21 and we get the finally output g to the 53.

131
00:10:24,943 --> 00:10:30,022
Okay, so this gives you an idea of how
this repeated squaring algorithm works.

132
00:10:30,022 --> 00:10:35,777
It's is quite an interesting algorithm and
it allows us to compute enormous powers of

133
00:10:35,777 --> 00:10:41,064
g very, very, very quickly. So the number
of iterations here, essentially, would be

134
00:10:41,064 --> 00:10:46,456
log base 2 of x. Okay. You notice the
number of iterations simply depends on the

135
00:10:46,456 --> 00:10:51,954
number of digits of x, which is basically
the log base 2 of x. So even if  x is a

136
00:10:51,954 --> 00:10:56,519
500 bit number in 500 multiplication,
well, 500 iterations, really 1,000

137
00:10:56,519 --> 00:11:01,736
multiplications because we have to square
and we have to accumulate. So in 1,000

138
00:11:01,736 --> 00:11:06,627
multiplications we'll be able to raise g
to the power of a 500 bit exponent.

139
00:11:06,627 --> 00:11:12,760
Okay so now we can summarize kind of the running times so suppose we

140
00:11:12,760 --> 00:11:17,680
have an N bit modulus capital N as we
said addition and subtraction in ZN takes

141
00:11:17,680 --> 00:11:22,157
linear time. Multiplication of just, you
know, as I said, Karatsuba's actually makes this

142
00:11:22,157 --> 00:11:26,897
more efficient, but for simplicity we'll
just say that it takes quadratic time. And

143
00:11:26,897 --> 00:11:31,579
then exponentiation, as I said, basically
takes log of x iterations, and at each

144
00:11:31,579 --> 00:11:35,509
iteration we basically do two
multiplications. So it's O(log (x))

145
00:11:35,509 --> 00:11:40,307
times the time to multiply. And let's say
that the time to multiply is quadratic. So

146
00:11:40,307 --> 00:11:44,758
the running time would be, really, N
squared log x. And since x is always gonna

147
00:11:44,758 --> 00:11:49,168
be less than N, by Fermat's theorem
there's no point in raising g to a power

148
00:11:49,168 --> 00:11:53,958
that's larger than the modulus. So x is
gonna be less than N. Let's suppose that x

149
00:11:53,958 --> 00:11:58,570
is also an N-bit integer, then, really
exponentiation is a cubic-time algorithm.

150
00:11:58,570 --> 00:12:02,970
Okay so that's what I wanted you to
remember, that exponentiation is actually

151
00:12:02,970 --> 00:12:07,163
a relatively slow. These days, it actually
takes a few microseconds on a modern

152
00:12:07,163 --> 00:12:11,259
computer. But still, microseconds for a,
for a, say a four gigahertz processor is

153
00:12:11,259 --> 00:12:15,092
quite a bit of work. And so just keep in
mind that all the exponentiation

154
00:12:15,092 --> 00:12:19,556
operations we talked about. For example,
for determining if a number is a quadratic

155
00:12:19,556 --> 00:12:23,600
residue or not, All those, all those
exponentiations basically take cubic time.

156
00:12:24,780 --> 00:12:29,677
Okay, so that completes our discussion of
arithmetic algorithms, and then in the

157
00:12:29,677 --> 00:12:34,078
next segment we'll start talking about
hard problems, modulo, primes and composites.

